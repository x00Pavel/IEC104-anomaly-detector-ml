{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pyshark\n",
    "from traceback import print_exc\n",
    "import csv\n",
    "import concurrent.futures\n",
    "from joblib import load\n",
    "import nest_asyncio\n",
    "\n",
    "DATA_DIR = join(\"data\")\n",
    "PCAP_D = join(DATA_DIR, \"pcap\")\n",
    "CSV_D = join(DATA_DIR, \"csv\")\n",
    "MODELS_D = join(DATA_DIR, \"models\")\n",
    "\n",
    "PCAP = {\"1\": join(PCAP_D, \"mega104-17-12-18.pcapng\"),\n",
    "        \"2\": join(PCAP_D, \"10122018-104Mega.pcapng\"),\n",
    "        \"3\": join(PCAP_D, \"10122018-104Mega-anomaly.pcapng\")}\n",
    "\n",
    "CSV = {\"1\": join(CSV_D, \"mega104-17-12-18.csv\"),\n",
    "       \"2\": join(CSV_D, \"10122018-104Mega.csv\"),\n",
    "       \"3\": join(CSV_D, \"10122018-104Mega-anomaly.csv\")}\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(num=1):\n",
    "    pcap_file = PCAP[str(num)]\n",
    "    print(f\"Reading from {pcap_file}\")\n",
    "    packets = pyshark.FileCapture(pcap_file)\n",
    "\n",
    "    parsed_data = [(\"asdu_len\", \"io_type\", \"type_id\", \"src\", \"dst\", \"interval\", \"relative_time_stamp\")]\n",
    "    \n",
    "    previous = 0\n",
    "    first_time_stamp = packets[0].sniff_time\n",
    "    relative_time = 0\n",
    "    interval = 0\n",
    "    hosts = {}\n",
    "    next_index = 0\n",
    "    for p in packets:\n",
    "        if \"iec60870_104\" not in [l.layer_name for l in p.layers]:\n",
    "            continue\n",
    "        \n",
    "        # Count time from the previous IEC 104 packet\n",
    "        if previous != 0:\n",
    "            interval = float((p.sniff_time - previous).total_seconds())\n",
    "            relative_time = (p.sniff_time - first_time_stamp).total_seconds()\n",
    "        if p.ip.src not in hosts.keys():\n",
    "            hosts[p.ip.src] = next_index\n",
    "            next_index += 1\n",
    "        if p.ip.dst not in hosts.keys():\n",
    "            hosts[p.ip.dst] = next_index\n",
    "            next_index += 1\n",
    "        \n",
    "        src = hosts[p.ip.src]\n",
    "        dst = hosts[p.ip.dst]\n",
    "        \n",
    "        previous = p.sniff_time\n",
    "        # Extract only one 'representative' for the current package\n",
    "        asdu_layer = p.get_multiple_layers(\"iec60870_asdu\")\n",
    "        if len(asdu_layer) == 0:\n",
    "            continue\n",
    "        asdu_layer = asdu_layer[0]\n",
    "\n",
    "        iec_header_layer = p.get_multiple_layers(\"iec60870_104\")\n",
    "        # Aggregate values if more then one header is present in the packet\n",
    "        iec_header = iec_header_layer[0]\n",
    "        try:\n",
    "            iec_header.apdulen = int(iec_header.apdulen)\n",
    "        except AttributeError:\n",
    "            # Not all APDU has valid apdulen attribute. Those packets in\n",
    "            # Wireshark displayed as a byte sequence, so this packet can\n",
    "            # be parsed\n",
    "            print(\"Error in converting the value in packet\")\n",
    "            print_exc()\n",
    "            print(p)\n",
    "            continue\n",
    "\n",
    "        if len(iec_header_layer) != 1:\n",
    "            for entry in iec_header_layer[1:]:\n",
    "                iec_header.apdulen += int(entry.apdulen)\n",
    "\n",
    "        try:\n",
    "            if asdu_layer:\n",
    "                parsed_data.append((iec_header.apdulen, asdu_layer.ioa, asdu_layer.typeid, src, dst, interval, relative_time))\n",
    "        except:\n",
    "            # Ignoring error if data can't be appended for some reasons.\n",
    "            print(\"Error in parsing the packet\")\n",
    "            print_exc()\n",
    "            print(p)\n",
    "\n",
    "    with open(CSV[str(num)], \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(parsed_data)\n",
    "\n",
    "    print(f\"CSV file is stored into {CSV[str(num)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(parse, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, nu, dataset):\n",
    "    model = join(MODELS_D, f\"one-class-svm-{model}-nu-{nu}.joblib\")\n",
    "\n",
    "    svm = load(model)\n",
    "    data = pd.read_csv(dataset).drop(columns=[\"relative_time_stamp\"])\n",
    "\n",
    "    prediction = svm.predict(data)\n",
    "    size = len(prediction)\n",
    "    t = [i for i in prediction if i == -1]\n",
    "    anomalies = len(t)\n",
    "    t = [i for i in prediction if i == 1]\n",
    "    ok = len(t)\n",
    "    perc_anom = anomalies/size\n",
    "\n",
    "    print(\"-\"*(len(f\"Total number of samples: {size}\") + 2))\n",
    "    print(f\"Datset: {dataset}\")\n",
    "    print(f\"Total number of samples: {size}\")\n",
    "    print(f\"Normal: {ok} ({100*(1-perc_anom):.2f}%)\")\n",
    "    print(f\"Anomalies: {anomalies} ({100*perc_anom:.2f}%)\")\n",
    "    print(\"-\"*(len(f\"Total number of samples: {size}\") + 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num=1, nu=0.018):\n",
    "    iec104 = pd.read_csv(CSV[str(num)], header=0, skipinitialspace=True)\n",
    "\n",
    "    iec104 = iec104.drop(columns=[\"relative_time_stamp\"])\n",
    "    x_train, x_test = train_test_split(iec104, train_size=2/3, test_size=1/3,\n",
    "                                    shuffle=False, random_state=0)\n",
    "    one_class_svm = OneClassSVM(nu=nu, kernel = 'rbf', gamma = 0.1).fit(x_train)\n",
    "    dump(one_class_svm, f\"{DATA_DIR}/models/one-class-svm-{num}-nu-{nu:.3f}.joblib\")\n",
    "    prediction = one_class_svm.predict(x_test)\n",
    "\n",
    "    size = len(prediction)\n",
    "    t = [i for i in prediction if i == -1]\n",
    "    anomalies = len(t)\n",
    "    t = [i for i in prediction if i == 1]\n",
    "    ok = len(t)\n",
    "    perc_anom = anomalies/size\n",
    "    \n",
    "    print(\"-\"*(len(f\"Datset: {CSV[str(num)]}\") + 2))\n",
    "    print(f\"Datset: {CSV[str(num)]}\")\n",
    "    print(f\"Nu is: {nu:.4f}\")\n",
    "    print(f\"Total number of samples: {size}\")\n",
    "    print(f\"Normal: {ok} ({100*(1-perc_anom):.2f}%)\")\n",
    "    print(f\"Anomalies: {anomalies} ({100*perc_anom:.2f}%)\")\n",
    "    print(\"-\"*(len(f\"Datset: {CSV[str(num)]}\") + 2))\n",
    "\n",
    "    return [nu, perc_anom, 1 - perc_anom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0170\n",
      "Total number of samples: 12554\n",
      "Normal: 12261 (97.67%)\n",
      "Anomalies: 293 (2.33%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0170\n",
      "Total number of samples: 22126\n",
      "Normal: 21712 (98.13%)\n",
      "Anomalies: 414 (1.87%)\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.017, 0.018711018711018712, 0.9812889812889813]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-shot training\n",
    "nu = 0.017\n",
    "create_model(1, nu)\n",
    "create_model(2, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0130\n",
      "Total number of samples: 12554\n",
      "Normal: 12157 (96.84%)\n",
      "Anomalies: 397 (3.16%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0150\n",
      "Total number of samples: 12554\n",
      "Normal: 12149 (96.77%)\n",
      "Anomalies: 405 (3.23%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0140\n",
      "Total number of samples: 12554\n",
      "Normal: 12231 (97.43%)\n",
      "Anomalies: 323 (2.57%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0160\n",
      "Total number of samples: 12554\n",
      "Normal: 12338 (98.28%)\n",
      "Anomalies: 216 (1.72%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0170\n",
      "Total number of samples: 12554\n",
      "Normal: 12261 (97.67%)\n",
      "Anomalies: 293 (2.33%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0180\n",
      "Total number of samples: 12554\n",
      "Normal: 12197 (97.16%)\n",
      "Anomalies: 357 (2.84%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0200\n",
      "Total number of samples: 12554\n",
      "Normal: 12337 (98.27%)\n",
      "Anomalies: 217 (1.73%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0190\n",
      "Total number of samples: 12554\n",
      "Normal: 12304 (98.01%)\n",
      "Anomalies: 250 (1.99%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0220\n",
      "Total number of samples: 12554\n",
      "Normal: 12252 (97.59%)\n",
      "Anomalies: 302 (2.41%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0230\n",
      "Total number of samples: 12554\n",
      "Normal: 12219 (97.33%)\n",
      "Anomalies: 335 (2.67%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0240\n",
      "Total number of samples: 12554\n",
      "Normal: 12252 (97.59%)\n",
      "Anomalies: 302 (2.41%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0250\n",
      "Total number of samples: 12554\n",
      "Normal: 12219 (97.33%)\n",
      "Anomalies: 335 (2.67%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0210\n",
      "Total number of samples: 12554\n",
      "Normal: 12262 (97.67%)\n",
      "Anomalies: 292 (2.33%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0260\n",
      "Total number of samples: 12554\n",
      "Normal: 12156 (96.83%)\n",
      "Anomalies: 398 (3.17%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0270\n",
      "Total number of samples: 12554\n",
      "Normal: 12125 (96.58%)\n",
      "Anomalies: 429 (3.42%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0280\n",
      "Total number of samples: 12554\n",
      "Normal: 12233 (97.44%)\n",
      "Anomalies: 321 (2.56%)\n",
      "---------------------------------------\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Datset: data/csv/mega104-17-12-18.csvDatset: data/csv/mega104-17-12-18.csv\n",
      "Nu is: 0.0290\n",
      "Total number of samples: 12554\n",
      "Normal: 12192 (97.12%)\n",
      "Anomalies: 362 (2.88%)\n",
      "---------------------------------------\n",
      "\n",
      "Nu is: 0.0300\n",
      "Total number of samples: 12554\n",
      "Normal: 12211 (97.27%)\n",
      "Anomalies: 343 (2.73%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0150\n",
      "Total number of samples: 22126\n",
      "Normal: 11585 (52.36%)\n",
      "Anomalies: 10541 (47.64%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0160\n",
      "Total number of samples: 22126\n",
      "Normal: 16649 (75.25%)\n",
      "Anomalies: 5477 (24.75%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0130\n",
      "Total number of samples: 22126\n",
      "Normal: 21947 (99.19%)\n",
      "Anomalies: 179 (0.81%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0190\n",
      "Total number of samples: 22126\n",
      "Normal: 21046 (95.12%)\n",
      "Anomalies: 1080 (4.88%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0140\n",
      "Total number of samples: 22126\n",
      "Normal: 17517 (79.17%)\n",
      "Anomalies: 4609 (20.83%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0170\n",
      "Total number of samples: 22126\n",
      "Normal: 21712 (98.13%)\n",
      "Anomalies: 414 (1.87%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0220\n",
      "Total number of samples: 22126\n",
      "Normal: 18887 (85.36%)\n",
      "Anomalies: 3239 (14.64%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0180\n",
      "Total number of samples: 22126\n",
      "Normal: 15991 (72.27%)\n",
      "Anomalies: 6135 (27.73%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0200\n",
      "Total number of samples: 22126\n",
      "Normal: 18310 (82.75%)\n",
      "Anomalies: 3816 (17.25%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0230\n",
      "Total number of samples: 22126\n",
      "Normal: 15911 (71.91%)\n",
      "Anomalies: 6215 (28.09%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0210\n",
      "Total number of samples: 22126\n",
      "Normal: 21758 (98.34%)\n",
      "Anomalies: 368 (1.66%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0240\n",
      "Total number of samples: 22126\n",
      "Normal: 15444 (69.80%)\n",
      "Anomalies: 6682 (30.20%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0260\n",
      "Total number of samples: 22126\n",
      "Normal: 17288 (78.13%)\n",
      "Anomalies: 4838 (21.87%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0280\n",
      "Total number of samples: 22126\n",
      "Normal: 19843 (89.68%)\n",
      "Anomalies: 2283 (10.32%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0250\n",
      "Total number of samples: 22126\n",
      "Normal: 16820 (76.02%)\n",
      "Anomalies: 5306 (23.98%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0270\n",
      "Total number of samples: 22126\n",
      "Normal: 14791 (66.85%)\n",
      "Anomalies: 7335 (33.15%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0290\n",
      "Total number of samples: 22126\n",
      "Normal: 15393 (69.57%)\n",
      "Anomalies: 6733 (30.43%)\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Nu is: 0.0300\n",
      "Total number of samples: 22126\n",
      "Normal: 15883 (71.78%)\n",
      "Anomalies: 6243 (28.22%)\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check different Nu parameter for the training of the data\n",
    "nu = 0.002\n",
    "result_pd_1 = []\n",
    "result_pd_2 = []\n",
    "nus = np.arange(0.013, 0.031, 0.001)\n",
    "\n",
    "def train(nu):\n",
    "    entry = (nu, *create_model(1, nu=nu)[1:])\n",
    "    result_pd_1.append(entry)\n",
    "    entry = (nu, *create_model(2, nu=nu)[1:])\n",
    "    result_pd_2.append(entry)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(nus)) as executor:\n",
    "        executor.map(train, nus)\n",
    "\n",
    "\n",
    "df_1 = pd.DataFrame(result_pd_1, columns=[\"nu\", \"anomalies_1\", \"ok_1\"])\n",
    "df_2 = pd.DataFrame(result_pd_2, columns=[\"nu\", \"anomalies_2\", \"ok_2\"])\n",
    "df_1.to_csv(join(CSV_D, \"pandas-df-1.csv\"))\n",
    "df_2.to_csv(join(CSV_D, \"pandas-df-2.csv\"))\n",
    "df_1_sort = df_1.sort_values(by=['ok_1'], ascending=False).reset_index(drop=True)\n",
    "df_2_sort = df_2.sort_values(by=[\"ok_2\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      nu  anomalies_1      ok_1  anomalies_2      ok_2\n",
      "0  0.013          NaN       NaN     0.008090  0.991910\n",
      "1  0.021     0.023260  0.976740     0.016632  0.983368\n",
      "2  0.017     0.023339  0.976661     0.018711  0.981289\n"
     ]
    }
   ],
   "source": [
    "df_1_sort = df_1.sort_values(by=['ok_1'], ascending=False).reset_index(drop=True)\n",
    "df_2_sort = df_2.sort_values(by=[\"ok_2\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "ok_1 = df_1_sort.loc[(df_1_sort[\"ok_1\"] > 0.97)]\n",
    "ok_2 = df_2_sort.loc[(df_2_sort[\"ok_2\"] > 0.97)]\n",
    "\n",
    "merged = pd.merge(left=ok_1, right=ok_2, how=\"right\", left_on=\"nu\", right_on=\"nu\")\n",
    "print(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       asdu_len  io_type  type_id  src  dst  interval  \\\n",
      "0            17    65537      122    0    1  0.000000   \n",
      "1            19    65537      120    1    0  0.000595   \n",
      "2            17    65537      122    0    1  0.000119   \n",
      "3            20    65537      121    1    0  0.000338   \n",
      "4            17    65537      122    0    1  0.062174   \n",
      "...         ...      ...      ...  ...  ...       ...   \n",
      "66372       533    65537      125    1    0  0.119856   \n",
      "66373        25       31       36    1    0  0.000348   \n",
      "66374        17    65537      124    0    1  0.000470   \n",
      "66375        18    65537      123    1    0  0.000341   \n",
      "66376        17    65537      124    0    1  0.000164   \n",
      "\n",
      "             relative_time_stamp  \n",
      "0     1900-01-01 00:00:00.000000  \n",
      "1     1900-01-01 00:00:00.000000  \n",
      "2     1900-01-01 00:00:00.000000  \n",
      "3     1900-01-01 00:00:00.000000  \n",
      "4     1900-01-01 00:00:00.000000  \n",
      "...                          ...  \n",
      "66372 1900-01-01 00:00:00.176010  \n",
      "66373 1900-01-01 00:00:00.176010  \n",
      "66374 1900-01-01 00:00:00.176010  \n",
      "66375 1900-01-01 00:00:00.176010  \n",
      "66376 1900-01-01 00:00:00.176010  \n",
      "\n",
      "[66377 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(CSV[\"2\"])\n",
    "data[\"relative_time_stamp\"] = pd.to_datetime(data[\"relative_time_stamp\"], format=\"%f\")\n",
    "# data = data.set_index(\"relative_time_stamp\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to intervals for 5 minutes\n",
    "for i, frame in enumerate(data.groupby([data['relative_time_stamp'],pd.Grouper(key=\"relative_time_stamp\",freq='Min')])):\n",
    "    frame[1].to_csv(join(CSV_D, \"intervals\", f\"frame-{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Datset: data/csv/10122018-104Mega.csv\n",
      "Total number of samples: 66377\n",
      "Normal: 64846 (97.69%)\n",
      "Anomalies: 1531 (2.31%)\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Datset: data/csv/10122018-104Mega-anomaly.csv\n",
      "Total number of samples: 66377\n",
      "Normal: 64826 (97.66%)\n",
      "Anomalies: 1551 (2.34%)\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# predict(1, 0.021)\n",
    "predict(2, 0.021, CSV[\"2\"])\n",
    "predict(2, 0.021, CSV[\"3\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d721b7aa69adfd7a06c5c74cab2c506a8d78a054b34212b5774bf472b17f188"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
